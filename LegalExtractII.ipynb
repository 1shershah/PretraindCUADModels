{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download atticus data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (35) TCP connection reset by peer\n",
      "unzip:  cannot find or open cuad-models/roberta-base.zip, cuad-models/roberta-base.zip.zip or cuad-models/roberta-base.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TheAtticusProject/cuad.git\n",
    "!mv cuad cuad-training\n",
    "!unzip cuad-training/data.zip -d cuad-data/\n",
    "# !mkdir cuad-models\n",
    "# !curl https://zenodo.org/record/4599830/files/roberta-base.zip?download=1 --output cuad-models/roberta-base.zip\n",
    "# !unzip cuad-models/roberta-base.zip -d cuad-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run batch predictions on \"roberta based\" models trained on cuad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
    "\n",
    "def run_prediction(question_texts, context_text, model_path='marshmellow77/roberta-base-cuad'):\n",
    "    ### Setting hyperparameters\n",
    "    max_seq_length = 512\n",
    "    doc_stride = 256\n",
    "    n_best_size = 1\n",
    "    max_query_length = 64\n",
    "    max_answer_length = 512\n",
    "    do_lower_case = False\n",
    "    null_score_diff_threshold = 0.0\n",
    "\n",
    "    # model_name_or_path = \"../cuad-models/roberta-base/\"\n",
    "\n",
    "    def to_list(tensor):\n",
    "        return tensor.detach().cpu().tolist()\n",
    "\n",
    "    config_class, model_class, tokenizer_class = (\n",
    "        AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer)\n",
    "    config = config_class.from_pretrained(model_path)\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        model_path, do_lower_case=True, use_fast=False)\n",
    "    model = model_class.from_pretrained(model_path, config=config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    processor = SquadV2Processor()\n",
    "    examples = []\n",
    "\n",
    "    for i, question_text in enumerate(question_texts):\n",
    "        example = SquadExample(\n",
    "            qas_id=str(i),\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=None,\n",
    "            start_position_character=None,\n",
    "            title=\"Predict\",\n",
    "            answers=None,\n",
    "        )\n",
    "\n",
    "        examples.append(example)\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        doc_stride=doc_stride,\n",
    "        max_query_length=max_query_length,\n",
    "        is_training=False,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=1,\n",
    "    )\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            example_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                eval_feature = features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "                output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "                all_results.append(result)\n",
    "\n",
    "    final_predictions = compute_predictions_logits(\n",
    "        all_examples=examples,\n",
    "        all_features=features,\n",
    "        all_results=all_results,\n",
    "        n_best_size=n_best_size,\n",
    "        max_answer_length=max_answer_length,\n",
    "        do_lower_case=do_lower_case,\n",
    "        output_prediction_file=None,\n",
    "        output_nbest_file=None,\n",
    "        output_null_log_odds_file=None,\n",
    "        verbose_logging=False,\n",
    "        version_2_with_negative=True,\n",
    "        null_score_diff_threshold=null_score_diff_threshold,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base model \"bert (trained on non legal data)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP].\n",
    "        \"\"\"\n",
    "\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                output = self.model(**chunk)\n",
    "                answer_start_scores, answer_end_scores = output['start_logits'] , output['end_logits']\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking how cuad questions were set - t set custom question similar fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./cuad-data/CUADv1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "questions = []\n",
    "for i, q in enumerate(data['data'][0]['paragraphs'][0]['qas']):\n",
    "    question = data['data'][0]['paragraphs'][0]['qas'][i]['question']\n",
    "    questions.append(question)\n",
    "contract = data['data'][0]['paragraphs'][0]['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the questions and the document that serves the asnwers to those question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename_with_questions = 'MSA/TermDefinitionswithQA.xlsx'\n",
    "termdefinitionqa = pd.read_excel(filename_with_questions)\n",
    "import glob \n",
    "import itertools\n",
    "import re \n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "filename = 'MSA/GenentechInc_Restated_MSSA_01Jan17.pdf'\n",
    "opfoldernname = filename.split(\".pdf\")[0]+\"/output/\"\n",
    "\n",
    "lines = []\n",
    "files = glob.glob(opfoldernname+ \"./page*inreadingorder*.txt\")\n",
    "files = [x[1] for x in sorted(zip(list(map(lambda x : int(x.split(\"page_\")[1].split(\"-pdf\")[0]), files)), files))]\n",
    "for fname in files:\n",
    "    with open(fname) as f:\n",
    "        lines.append(f.readlines())\n",
    "lines = list(itertools.chain.from_iterable(lines))\n",
    "msa_text = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SUB-CATGEORY</th>\n",
       "      <th>DEFINITION</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INVESTMENTS</td>\n",
       "      <td>Costs to fulfill a contract (606)</td>\n",
       "      <td>Contract includes provision requiring PPD, at ...</td>\n",
       "      <td>What are the costs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INVESTMENTS</td>\n",
       "      <td>Free Services (606)</td>\n",
       "      <td>Contract includes provision setting out that P...</td>\n",
       "      <td>What are the free or discounted services that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CATEGORY                       SUB-CATGEORY  \\\n",
       "0  INVESTMENTS  Costs to fulfill a contract (606)   \n",
       "1  INVESTMENTS                Free Services (606)   \n",
       "\n",
       "                                          DEFINITION  \\\n",
       "0  Contract includes provision requiring PPD, at ...   \n",
       "1  Contract includes provision setting out that P...   \n",
       "\n",
       "                                            Question  \n",
       "0                                 What are the costs  \n",
       "1  What are the free or discounted services that ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termdefinitionqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = termdefinitionqa.apply( lambda x : x['Question']+\"?\" + \" Detail: \"+ x['DEFINITION'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run predictions on two Roberta and Deberta models on quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 45/45 [14:47<00:00, 19.72s/it]  \n",
      "add example index and unique id: 100%|██████████| 45/45 [00:00<00:00, 5925.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 45/45 [13:18<00:00, 17.73s/it]  \n",
      "add example index and unique id: 100%|██████████| 45/45 [00:00<00:00, 6242.56it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = termdefinitionqa.apply( lambda x : x['Question']+\"?\" + \" Detail: \"+ x['DEFINITION'],axis=1)\n",
    "predictions_I = run_prediction(questions, msa_text,\"Rakib/roberta-base-on-cuad\")\n",
    "predictions_II = run_prediction(questions, msa_text,\"akdeniz27/deberta-v2-xlarge-cuad\")\n",
    "predictions_III = []\n",
    "# reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "# for question in termdefinitionqa.Question:\n",
    "#     reader.tokenize(question, msa_text)\n",
    "#     predictions_III.append(reader.get_answer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"Q\": questions , \"P1\":predictions_II.values(),\"P2\":predictions_I.values(),\"P3\":\"\"})\n",
    "output[\"Answer\"] =  output.\\\n",
    "apply( lambda x : x[\"P1\"] if ( (len(x[\"P1\"])>1) and (x[\"P1\"]!='empty'))\\\n",
    "       else x[\"P2\"] if ( (len(x[\"P1\"])>1) and (x[\"P1\"]!='empty')) \\\n",
    "       else x[\"P3\"] , axis=1  )\n",
    "output.to_csv(opfoldernname+\"answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict based on last hidden layer embedding -  mpnet and deepset-roberta based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, pipeline\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# import textract\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\").to(device).eval()\n",
    "tokenizer_ans = AutoTokenizer.from_pretrained(\"deepset/roberta-large-squad2\")\n",
    "model_ans = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-large-squad2\").to(device).eval()\n",
    "if device == 'cuda:0':\n",
    "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans,device = 0)\n",
    "else:\n",
    "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans)\n",
    "    \n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:,0]\n",
    "\n",
    "def encode_query(query):\n",
    "    encoded_input = tokenizer(query, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    embeddings = cls_pooling(model_output)\n",
    "\n",
    "    return embeddings.cpu()\n",
    "\n",
    "\n",
    "def encode_docs(docs,maxlen = 64, stride = 32):\n",
    "    encoded_input = []\n",
    "    embeddings = []\n",
    "    spans = []\n",
    "    file_names = []\n",
    "    name, text = docs\n",
    "    \n",
    "    text = text.split(\" \")\n",
    "    if len(text) < maxlen:\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
    "        spans.append(temp_text)\n",
    "        file_names.append(name)\n",
    "\n",
    "    else:\n",
    "        num_iters = int(len(text)/maxlen)+1\n",
    "        for i in range(num_iters):\n",
    "            if i == 0:\n",
    "                temp_text = \" \".join(text[i*maxlen:(i+1)*maxlen+stride])\n",
    "            else:\n",
    "                temp_text = \" \".join(text[(i-1)*maxlen:(i)*maxlen][-stride:] + text[i*maxlen:(i+1)*maxlen])\n",
    "\n",
    "            encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
    "            spans.append(temp_text)\n",
    "            file_names.append(name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoded in tqdm(encoded_input): \n",
    "            model_output = model(**encoded, return_dict=True)\n",
    "            embeddings.append(cls_pooling(model_output))\n",
    "    \n",
    "    embeddings = np.float32(torch.stack(embeddings).transpose(0, 1).cpu())\n",
    "    \n",
    "#     np.save(\"emb_{}.npy\".format(name),dict(zip(list(range(len(embeddings))),embeddings))) \n",
    "#     np.save(\"spans_{}.npy\".format(name),dict(zip(list(range(len(spans))),spans)))\n",
    "#     np.save(\"file_{}.npy\".format(name),dict(zip(list(range(len(file_names))),file_names)))\n",
    "    \n",
    "    return embeddings, spans, file_names\n",
    "   \n",
    "def predict(query,text):\n",
    "#     name_to_save = data.name.split(\"/\")[-1].split(\".\")[0][:-8]\n",
    "    name_to_save = 'msa'\n",
    "#     st = str([query,name_to_save])\n",
    "#     st_hashed = str(hashlib.sha256(st.encode()).hexdigest()) #just to speed up examples load\n",
    "#     hist = st + \" \" + st_hashed \n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     try:\n",
    "#         df = pd.read_csv(\"{}.csv\".format(st_hashed))\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(st)\n",
    "\n",
    "#     if name_to_save+\".txt\" in os.listdir():\n",
    "#         doc_emb = np.load('emb_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "#         doc_text = np.load('spans_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "#         file_names_dicto = np.load('file_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "        \n",
    "#         doc_emb = np.array(list(doc_emb.values())).reshape(-1,768)\n",
    "#         doc_text = list(doc_text.values())\n",
    "#         file_names = list(file_names_dicto.values())\n",
    "    \n",
    "#     else:\n",
    "#         text = textract.process(\"{}\".format(data.name)).decode('utf8')\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\" . \",\" \")\n",
    "\n",
    "    doc_emb, doc_text, file_names = encode_docs((name_to_save,text),maxlen = 64, stride = 32)\n",
    "\n",
    "    doc_emb = doc_emb.reshape(-1, 768)\n",
    "    with open(\"{}.txt\".format(name_to_save),\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    start = time.time()\n",
    "    query_emb = encode_query(query)\n",
    "    \n",
    "    scores = np.matmul(query_emb, doc_emb.transpose(1,0))[0].tolist()\n",
    "    doc_score_pairs = list(zip(doc_text, scores, file_names))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    k = 5\n",
    "    probs_sum = 0\n",
    "    probs = softmax(sorted(scores,reverse = True)[:k])\n",
    "    table = {\"Passage\":[],\"Answer\":[],\"Probabilities\":[],\"Source\":[]}\n",
    "    \n",
    "    for i, (passage, _, names) in enumerate(doc_score_pairs[:k]):\n",
    "        passage = passage.replace(\"\\n\",\"\")\n",
    "        passage = passage.replace(\" . \",\" \")\n",
    "        \n",
    "        if probs[i] > 0.1 or (i < 3 and probs[i] > 0.05): #generate answers for more likely passages but no less than 2\n",
    "            QA = {'question':query,'context':passage}\n",
    "            ans = pipe(QA)\n",
    "            probabilities = \"P(a|p): {}, P(a|p,q): {}, P(p|q): {}\".format(round(ans[\"score\"],5), \n",
    "                                                                          round(ans[\"score\"]*probs[i],5), \n",
    "                                                                          round(probs[i],5))\n",
    "            passage = passage.replace(str(ans[\"answer\"]),str(ans[\"answer\"]).upper()) \n",
    "            table[\"Passage\"].append(passage)\n",
    "            table[\"Passage\"].append(\"---\")\n",
    "            table[\"Answer\"].append(str(ans[\"answer\"]).upper())\n",
    "            table[\"Answer\"].append(\"---\")\n",
    "            table[\"Probabilities\"].append(probabilities)\n",
    "            table[\"Probabilities\"].append(\"---\")\n",
    "            table[\"Source\"].append(names)\n",
    "            table[\"Source\"].append(\"---\")\n",
    "        else:\n",
    "            table[\"Passage\"].append(passage)\n",
    "            table[\"Passage\"].append(\"---\")\n",
    "            table[\"Answer\"].append(\"no_answer_calculated\")\n",
    "            table[\"Answer\"].append(\"---\")\n",
    "            table[\"Probabilities\"].append(\"P(p|q): {}\".format(round(probs[i],5)))\n",
    "            table[\"Probabilities\"].append(\"---\")\n",
    "            table[\"Source\"].append(names)\n",
    "            table[\"Source\"].append(\"---\")\n",
    "    df = pd.DataFrame(table)\n",
    "    print(\"time: \"+ str(time.time()-start))\n",
    "    \n",
    "#     with open(\"HISTORY.txt\",\"a\", encoding = \"utf-8\") as f:\n",
    "#         f.write(hist)\n",
    "#         f.write(\" \" + str(current_time))\n",
    "#         f.write(\"\\n\")\n",
    "#         f.close()\n",
    "#     df.to_csv(\"{}.csv\".format(st_hashed), index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparision to Deepset prediction\n",
    "### Deepset performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "termdefinitionqa = pd.read_excel(filename_with_questions)\n",
    "import glob \n",
    "import itertools\n",
    "import re \n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "predictions_III= {}\n",
    "questions = termdefinitionqa.apply( lambda x : x['Question']+\"?\" + \" Detail: \"+ x['DEFINITION'],axis=1)\n",
    "for q in termdefinitionqa.Question:\n",
    "    predictions_III[q] = predict(q+\"?\",msa_text) \n",
    "passages = []\n",
    "for k in predictions_III.keys():\n",
    "    op = predictions_III[k].query(\"Answer!='---' and Answer!='no_answer_calculated' \")\n",
    "    op['Prob'] = op.Probabilities.apply(lambda x : float(x.split(\",\")[0].split(\"P(a|p):\")[1]) )\n",
    "    passages.append( ((op.sort_values([\"Prob\"],ascending=False).iloc[0].Passage),\n",
    "    (op.sort_values([\"Prob\"],ascending=False).iloc[0].Prob)))\n",
    "pd.DataFrame(passages).to_csv(opfoldernname+\"answer2.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MSA/GenentechInc_Restated_MSSA_01Jan17/output/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat([pd.read_csv(opfoldernname+\"/answer.csv\", index_col=0,keep_default_na=False).\\\n",
    "drop([\"P3\",\"Answer\"],axis=1),\n",
    "           pd.read_csv(opfoldernname+\"/answer2.csv\" , index_col=0,keep_default_na=False).\\\n",
    "rename({'0':'P3','1':'PROB'},axis=1)],axis=1)\n",
    "final[\"Answer\"] =  final.\\\n",
    "apply( lambda x : x[\"P1\"] if ( (len(x[\"P1\"])>1) and (x[\"P1\"]!='empty'))\\\n",
    "       else x[\"P2\"] if ( (len(x[\"P1\"])>1) and (x[\"P1\"]!='empty')) \\\n",
    "       else x[\"P3\"] if x['PROB']>0.00001 else \"no-answer\" , axis=1  )\n",
    "final[[\"Q\",\"Answer\",\"PROB\"]].to_csv(opfoldernname+\"finalop.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legaltermextract",
   "language": "python",
   "name": "legaltermextract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
